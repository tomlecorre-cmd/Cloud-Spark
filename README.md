# ðŸ›’ Analyse Big Data & Segmentation Client (PySpark)

![Python](https://img.shields.io/badge/Python-3.x-blue?style=flat&logo=python)
![Spark](https://img.shields.io/badge/Apache_Spark-PySpark-orange?style=flat&logo=apachespark)
![Machine Learning](https://img.shields.io/badge/ML-Clustering%20%26%20Classification-green)

## ðŸ« Contexte AcadÃ©mique
Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du **DU Data Analytics** Ã  l'**UniversitÃ© Paris 1 PanthÃ©on-Sorbonne** (Cours de Cloud Computing).

L'objectif est de dÃ©montrer la capacitÃ© Ã  traiter un grand volume de donnÃ©es transactionnelles (Big Data) pour en extraire de la valeur dÃ©cisionnelle en utilisant un environnement distribuÃ©.

---

## ðŸ” Que fait ce projet ? (Vue d'ensemble)

Ce projet simule le travail d'un Data Scientist dans une entreprise de e-commerce. Nous partons d'un fichier brut contenant 500 000 transactions pour rÃ©pondre Ã  deux questions business majeures :
1.  **Qui sont nos clients ?** (Segmentation pour adapter le marketing).
2.  **Qui va dÃ©penser beaucoup d'argent ?** (PrÃ©diction pour cibler les futurs VIP).

Le projet suit un pipeline de donnÃ©es complet en 3 Ã©tapes :

### 1ï¸âƒ£ Data Engineering (Nettoyage & ETL)
Les donnÃ©es rÃ©elles sont souvent "sales". Avant d'analyser, nous avons dÃ» :
* Supprimer les transactions orphelines (sans identifiant client).
* Filtrer les annulations de commandes (quantitÃ©s nÃ©gatives) qui faussent le Chiffre d'Affaires.
* Convertir les formats de dates et de prix pour qu'ils soient lisibles par Spark.
* **RÃ©sultat :** Nous passons de 541 000 lignes brutes Ã  **397 000 donnÃ©es fiables**.

### 2ï¸âƒ£ Segmentation Client (Clustering RFM)
Pour comprendre les comportements, nous avons utilisÃ© la mÃ©thode marketing **RFM**. Chaque client est notÃ© sur :
* **R**Ã©cence : Date de son dernier achat.
* **F**rÃ©quence : Combien de fois a-t-il achetÃ© ?
* **M**ontant : Combien a-t-il dÃ©pensÃ© au total ?

En appliquant l'algorithme **Bisecting KMeans**, nous avons dÃ©couvert 3 groupes de clients :
* ðŸ”´ **Les Inactifs :** N'ont pas achetÃ© depuis longtemps (Risque de dÃ©part).
* ðŸ”µ **Les RÃ©guliers :** AchÃ¨tent de temps en temps, panier moyen.
* ðŸŸ¢ **Les VIP :** AchÃ¨tent trÃ¨s souvent et dÃ©pensent beaucoup (CÅ“ur de cible).

### 3ï¸âƒ£ PrÃ©diction (Machine Learning SupervisÃ©)
Nous avons entraÃ®nÃ© une Intelligence Artificielle (RÃ©gression Logistique) pour qu'elle apprenne Ã  reconnaÃ®tre automatiquement un client Ã  fort potentiel.
* Le modÃ¨le prÃ©dit avec **98% de prÃ©cision** si un client sera un "Gros DÃ©pensier" (> 500â‚¬).

---

## ðŸ›  Technologies UtilisÃ©es
* **Langage :** Python
* **Big Data Framework :** Apache Spark (via l'API **PySpark**)
* **Machine Learning (Spark MLlib) :**
    * `VectorAssembler` & `StandardScaler` (PrÃ©paration des features)
    * `BisectingKMeans` (Clustering Non-SupervisÃ©)
    * `LogisticRegression` (Classification SupervisÃ©e)
* **Environnement :** Google Colab / Jupyter Notebook

## ðŸ“‚ Structure du Repository
```text
â”œâ”€â”€ Projet_Cloud.ipynb      # Le Notebook complet (Code commentÃ© Ã©tape par Ã©tape)
â”œâ”€â”€ cloudspark.pdf          # PPT du projet
â””â”€â”€ README.md               # Documentation du projet
